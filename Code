# ===================== ONE CELL: Train + Normalized Confusion Matrix + BEST-1 Prediction (Clean label) =====================

!pip -q install kaggle tensorflow scikit-learn matplotlib seaborn numpy

from google.colab import files
files.upload()  # upload kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d jessicali9530/stanford-dogs-dataset -p /content --unzip -q

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.utils import load_img, img_to_array
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

print("GPU Available:", tf.config.list_physical_devices('GPU'))

# ---- Dataset folder (robust to nesting) ----
DATA_DIR_CANDIDATES = ["images/Images", "images/images/Images"]
DATA_DIR = next((p for p in DATA_DIR_CANDIDATES if os.path.isdir(p)), None)
assert DATA_DIR is not None, f"Could not find Images folder. Tried: {DATA_DIR_CANDIDATES}"
print("Using DATA_DIR =", DATA_DIR)

# ---- Params ----
IMG_SIZE = 160
BATCH_SIZE = 32
SEED = 1337

# ---- Data loaders ----
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,
    validation_split=0.2,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,
    validation_split=0.2
)

train_data = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="training",
    seed=SEED
)

val_data = val_datagen.flow_from_directory(
    DATA_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    subset="validation",
    seed=SEED,
    shuffle=False
)

# ---- Index -> class label ----
idx_to_class = {v: k for k, v in val_data.class_indices.items()}
class_names = [idx_to_class[i] for i in range(len(idx_to_class))]

def clean_breed_label(label: str) -> str:
    # Example folder: "n02099601-golden_retriever" -> "golden retriever"
    label = label.split("-", 1)[-1]      # drop prefix if present
    label = label.replace("_", " ")      # prettier [web:185]
    return label.strip()

# ---- Model ----
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights="imagenet"
)
base_model.trainable = False

inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.4)(x)
outputs = layers.Dense(train_data.num_classes, activation="softmax")(x)
model = tf.keras.Model(inputs, outputs)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.2),
]

print("\n--- Phase 1: train head (frozen base) ---")
hist1 = model.fit(train_data, validation_data=val_data, epochs=10, callbacks=callbacks)

print("\n--- Phase 2: fine-tune last layers ---")
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)
hist2 = model.fit(train_data, validation_data=val_data, epochs=10, callbacks=callbacks)

# ---- Curves ----
def plot_hist(h, title_prefix=""):
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    plt.plot(h.history["accuracy"], label="Train Acc")
    plt.plot(h.history["val_accuracy"], label="Val Acc")
    plt.legend()
    plt.title(f"{title_prefix}Accuracy")

    plt.subplot(1,2,2)
    plt.plot(h.history["loss"], label="Train Loss")
    plt.plot(h.history["val_loss"], label="Val Loss")
    plt.legend()
    plt.title(f"{title_prefix}Loss")
    plt.show()

plot_hist(hist1, "Phase1 ")
plot_hist(hist2, "Phase2 ")

# ---- Normalized confusion matrix ----
val_data.reset()
y_prob = model.predict(val_data, steps=len(val_data), verbose=1)
y_pred = np.argmax(y_prob, axis=1)
y_true = val_data.classes

cm_norm = confusion_matrix(y_true, y_pred, normalize="true")

plt.figure(figsize=(12, 12))
disp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=class_names)
disp.plot(include_values=False, xticks_rotation=90, cmap="Blues", ax=plt.gca())
plt.title("Normalized Confusion Matrix (Validation, row-normalized)")
plt.tight_layout()
plt.show()

print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))

# ---- BEST-1 prediction on uploaded image (clean label) ----
print("\nUpload a dog image for BEST-1 prediction:")
uploaded = files.upload()
img_path = next(iter(uploaded.keys()))

img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))
x = img_to_array(img)
x = np.expand_dims(x, axis=0)
x = tf.keras.applications.mobilenet_v2.preprocess_input(x)

probs = model.predict(x, verbose=0)[0]
best_idx = int(np.argmax(probs))
raw_label = class_names[best_idx]
pretty_label = clean_breed_label(raw_label)
best_conf = float(probs[best_idx])

plt.figure(figsize=(4,4))
plt.imshow(img)
plt.axis("off")
plt.title("Uploaded image")
plt.show()

print("Best-1 prediction:")
print(f"{pretty_label}  {best_conf*100:.2f}%")
